:sectnums:
= Improve ML performance

== Evaluate the performance of a Classification Model

=== Accuracy
 accuracy = number of correct predictions / total number of predictions

* Pros: A single number measure
* Cons: Accuracy is not the best way to measure performance if unbalanced class
* For instance: balance class (50% cheese lovers and 50% cheese haters) and unbalanced classes (1 winner and 45 million loosers)

=== Confusion Matrix
* Pros: useful table for seeing the detail (correct and incorrect classification - True Positif (TP), False Positif (FP))
* Cons: Not a single number
=== Precision / Recall

=== Precision - penalizes models with FP
 Precision = (TP / (TP + FP) (amongst predicted positive)
 Cider manufacturer wants to maximize precision - who wants only good apples (minimum of bad apples)

=== Recall - penalizes models with FN
 Recall = TP / (TP + FN) (amongst real positive)
 Low-cost supermarket wants to maximize recall -  who don't want to throw away good apples (mimimum of good apples to throw away)

=== F1 - balances both Precision and Recall
 F1 = 2 * (precision * recall)/(precision + recall)
 
=== ROC (Receiver Operating Characteristic) Curve
* ROC curve shows how effectively a model can split binary classes
* Logistic Regresssion predict a probability a data point belongs to a specific class (78% is likely to be spam)
* ROC allows to move the threshold (probability) and then get another confuxion matrix
* Other measures: 
** True positive rate: TPR =  TP / (TP + FN) - True positif amongst all the positif predicted
** False positive rate FPR = FP / (FP + TN) - False positif amongst all the negative predicted
* ROC curbe represents TPR by FPR -> good model goes along the top left corner
* AUC = numerical measure for comparring ROC curves

=== sklearn api ===
* accuracy_score(y_test, predictions)
* confusion_matrix(y_test, predictions)
* classifcaton_report(y_test, predictions, target_names = data.target_names)
* plotRocAuc(model, X_test, y_test)

=== Balance and unbalanced classes ===
* Balance unbalanced classes by 
** oversampling smaller class (add copies of a random selection of smaller class)
 # Oversample malignant
 Xy_train_malignant_oversampled = resample(Xy_train_malignant, replace=True, n_samples=Xy_train_benign_count)
** undersampling larger class (removes data points at random of larger class)


== Evaluate the performance of a Regression Model
 Example of determining a tree age by looking at his diameter
 
=== Mean Absolute Error
 
* MAE (Mean absolute error) : how far in the model is from the reality in absolute value by get the average of gap
 MAE = sum (y-y^) / N
 Example for the tree : MAE : 3.4 years
 Pros: unis is in the same as the feature to be predicted
 Cons: does not tell if a model is good or not
 
* RMSE (Root mean square error): similar to MAE (absolute value replaced by squaring the error)
 RMSE vs MAE : RMSE penalizes large errors more than MAE (because of the square)
 Pros: unit is in target unit
 Cons: does not say whether it's a good or not

* R2 (squarred R)
** Compare the model with a very simplified model
** Example: Computes how much better the regression line fits the data than the mean line
** R2 = (var(mean) - var(regressionline)) / var(mean)
** R2 between 0 and 1. 
*** 0 means the model has the same variance as an oversimplified model --> does not bring anything good
*** 1 means the model fits perfectly with the data
*** <0 : model is worse than just predicting the mean model  
     
* sklearn api
** MAE: mean_absolute_error(actuals, predictions)
** RMSE : sqrt(mean_squared_error(actuals, predictions))
** R2 : r2_score(actuals, predictions)


== Exercices with Classification
* LogisticRegression() : predict a probability to belong to a given class with a threshold. Special case of generalized linear models with a Bernouilli conditional distribution
* DecisionTree()
* GaussianNB()
* SVC()

== Regularization
* Tune model further with fighting against overfitting especially with complex models
 R2 great for train data but very bad for test data --> overfit
* Minimize Loss function; min L = sum (y-yÃ®)^2
* 2 types of regularization on linear regression: 
** Ridge regularization (add penalty alpha * Sum(m^2)) -> performs better when features of roughly equal importance
** Lasso regularization (add penalty alpha * sum (abs(m)) -> useful when features vary significantly and will to do a feature selection
** sklearn function
*** Ridge(alpha=2) -> set lever to alpha = 2 but using grid search cross-validation allows to find best setting for hyperparameters
*** RidgeCV(cv=kfold) (kfold = KFold(....))

== Exercice with regression
 Example: bikes rental
* Remove variables that are strongly correlated to each other: cf = correlatedFeatures(X_train, 0.85)
* Remove variables connected to the target (total count)
* one-hot-encode with pd.get_dummies(weekdays values into column, season into column)
* Remove variables with low variance. 
 X.var()
 sel = VarianceThreshold(threshold=(0.01))
 sel.fit(X_train)
* Apply RFE (recursive feature elimination). Loop with n features and decrease the number of features and check all the score
 
