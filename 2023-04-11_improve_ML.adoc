:sectnums:
= Improve ML performance

== Evaluate the performance of a Classification Model

=== Accuracy
 accuracy = number of correct predictions / total number of predictions

* Pros: A single number measure
* Cons: Accuracy is not the best way to measure performance if unbalanced class
* For instance: balance class (50% cheese lovers and 50% cheese haters) and unbalanced classes (1 winner and 45 million loosers)

=== Confusion Matrix
* Pros: useful table for seeing the detail (correct and incorrect classification - True Positif (TP), False Positif (FP))
* Cons: Not a single number
=== Precision / Recall

=== Precision - penalizes models with FP
 Precision = (TP / (TP + FP) (amongst predicted positive)
 Cider manufacturer wants to maximize precision - who wants only good apples (minimum of bad apples)

=== Recall - penalizes models with FN
 Recall = TP / (TP + FN) (amongst real positive)
 Low-cost supermarket wants to maximize recall -  who don't want to throw away good apples (mimimum of good apples to throw away)

=== F1 - balances both Precision and Recall
 F1 = 2 * (precision * recall)/(precision + recall)
 
=== ROC (Receiver Operating Characteristic) Curve
* ROC curve shows how effectively a model can split binary classes
* Logistic Regresssion predict a probability a data point belongs to a specific class (78% is likely to be spam)
* ROC allows to move the threshold (probability) and then get another confuxion matrix
* Other measures: 
** True positive rate: TPR =  TP / (TP + FN) - True positif amongst all the positif predicted
** False positive rate FPR = FP / (FP + TN) - False positif amongst all the negative predicted
* ROC curbe represents TPR by FPR -> good model goes along the top left corner
* AUC = numerical measure for comparring ROC curves

=== sklearn api ===
* accuracy_score(y_test, predictions)
* confusion_matrix(y_test, predictions)
* classifcaton_report(y_test, predictions, target_names = data.target_names)
* plotRocAuc(model, X_test, y_test)

=== Balance and unbalanced classes ===
* Balance unbalanced classes by 
** oversampling smaller class (add copies of a random selection of smaller class)
 # Oversample malignant
 Xy_train_malignant_oversampled = resample(Xy_train_malignant, replace=True, n_samples=Xy_train_benign_count)
** undersampling larger class (removes data points at random of larger class)


== Evaluate the performance of a Regression Model
 Example of determining a tree age by looking at his diameter
=== Mean Absolute Error
 
* MAE (Mean absolute error) : how far in the model is from the reality in absolute value by get the average of gap
 MAE = sum (y-y^) / N
 Example for the tree : MAE : 3.4 years
 Pros: unis is in the same as the feature to be predicted
 Cons: does not tell if a model is good or not
 
* RMSE (Root mean square error): similar to MAE (absolute value replaced by squaring the error)
 RMSE vs MAE : RMSE penalizes large errors more than MAE (because of the square)
 Pros: unit is in target unit
 Cons: does not say whether it's a good or not

* R2 (squarred R)
Compare the model with a very simplified model
Example: Computes how much better the regression line fits the data than the mean line
 Pros: 
 Pr
 
